{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import sys\n",
    "import string\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to read in the data from the zipped data. Returns an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATORY ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the first record in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewTime': '09 26, 2013',\n",
       " 'reviewText': \"The model in this picture has them rolled up at the top because they are actually very high waisted! that's my only complaint though, because they are very good quality, and fit really well! I am 5'2&#34; 120lbs with thick thighs and i love them i can't wait to wear them out!\",\n",
       " 'helpful': {'nHelpful': 0, 'outOf': 0},\n",
       " 'reviewerID': 'U490934656',\n",
       " 'reviewHash': 'R798569390',\n",
       " 'categories': [['Clothing, Shoes & Jewelry', 'Women'],\n",
       "  ['Clothing, Shoes & Jewelry',\n",
       "   'Novelty, Costumes & More',\n",
       "   'Novelty',\n",
       "   'Clothing',\n",
       "   'Women',\n",
       "   'Leggings']],\n",
       " 'unixReviewTime': 1380153600,\n",
       " 'itemID': 'I402344648',\n",
       " 'rating': 4.0,\n",
       " 'summary': 'High Waisted',\n",
       " 'categoryID': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(readGz(\"train.json.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a legend of the data fields that appear:\n",
    "- __reviewTime__: the time in mm/dd/yyyy format of when the review was made.\n",
    "- __reviewText__: the text of the review in a string format.\n",
    "- __helpful__: the number of people that upvoted the review as helpful.\n",
    "- __reviewerID__: a unique string that represents the reviewer\n",
    "- __reviewHash__: a hash number to anonymise the review\n",
    "- __categories__: the different categories the item falls under \n",
    "- __unixReviewTime__: seconds passed since Jan 1, 1970 (UTC)\n",
    "- __itemID__: a unique string that represents the item\n",
    "- __rating__: an integer rating the reviewer gave the item\n",
    "- __summary__: a short summary string of the item being reviewed\n",
    "- __categoryID__: 0 = Women, 1 = Men, 2 = Girl, 3 = Boy, 4 = Infant\n",
    "- __price__: the price of the item (not available for all items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the data and counting the number of items in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum = []\n",
    "item_counts = {'0':0, '1':0, '2':0,'3':0,'4':0}\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "        datum.append([l['reviewText'], l['categoryID'], l['reviewerID']])\n",
    "        item_counts[str(l['categoryID'])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(0,5),list(item_counts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 141398, '1': 51416, '2': 2329, '3': 1881, '4': 2976}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the object above we can see that womens clothing was the most common with 141398 putchases, followed by mens, then infants, girls and boys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the 200,000 data points into a training and test set, allocating 175,000 points in the training set and the remaining 25,000 points in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "datum_train = datum[:175000]\n",
    "datum_valid = datum[175000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a very basic baseline where we predict the category of the item based on certain keywords appearing in the review. <br>\n",
    "If the 'wife' occurs in the review, we will predict the category to be womens, 'husband' for men, 'daughter' for girls, 'son' for boys and 'baby' for infants. <br> <br>\n",
    "Incase none of the words occur we will predict the category to be womens as it was the most common category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6727142857142857\n"
     ]
    }
   ],
   "source": [
    "### Category prediction baseline: Just consider some of the most common words from each category\n",
    "\n",
    "catDict = {\n",
    "  \"Women\": 0,\n",
    "  \"Men\": 1,\n",
    "  \"Girls\": 2,\n",
    "  \"Boys\": 3,\n",
    "  \"Baby\": 4\n",
    "}\n",
    "\n",
    "num = 0\n",
    "den = 0\n",
    "\n",
    "for l in datum_train:\n",
    "    # If there's no evidence, just choose the most common category in the dataset\n",
    "    cat = catDict['Women'] \n",
    "    words = l[0].lower()\n",
    "    if 'wife' in words:\n",
    "        cat = catDict['Women']\n",
    "    if 'husband' in words:\n",
    "        cat = catDict['Men']\n",
    "    if 'daughter' in words:\n",
    "        cat = catDict['Girls']\n",
    "    if 'son' in words:\n",
    "        cat = catDict['Boys']\n",
    "    if 'baby' in words:\n",
    "        cat = catDict['Baby']\n",
    "    if cat == l[1]:\n",
    "        num += 1\n",
    "    den += 1\n",
    "\n",
    "print(num/den)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of our basic baseline was __67.27%__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTIVE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a user dictionary, which for each userID contains the itemID, categoryID and rating of every item they have purchased. <br><br>\n",
    "Similarly we will have an item dictionary, which for each itemID contains userID, categoryID, rating and proportion of useful upvotes for all purchases of the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cat_d = {}\n",
    "item_cat_d = {}\n",
    "counter = 0\n",
    "p = 0\n",
    "for l in readGz(\"train.json.gz\"):\n",
    "    if counter < 175000:\n",
    "        cat_ID = l['categoryID']\n",
    "        item = l['itemID']\n",
    "        user = l['reviewerID']\n",
    "        helpful = l['helpful']\n",
    "        \n",
    "        if helpful['outOf'] == 0:\n",
    "            helpful = 0\n",
    "        else:\n",
    "            helpful = helpful['nHelpful'] / helpful['outOf']\n",
    "        \n",
    "        price = 0 \n",
    "        if 'price' in l.keys():\n",
    "            price = l['price']\n",
    "        rating = l['rating']\n",
    "        \n",
    "        \n",
    "        tup = (user,cat_ID, rating, helpful)\n",
    "        if item not in item_cat_d.keys():\n",
    "            item_cat_d[item] = set([tup])\n",
    "        else:\n",
    "            item_cat_d[item].add(tup)\n",
    "\n",
    "        tup = (item, cat_ID, rating)\n",
    "        if user not in user_cat_d.keys():\n",
    "            \n",
    "            user_cat_d[user] = set([tup])\n",
    "        else:\n",
    "            user_cat_d[user].add(tup)\n",
    "        counter += 1\n",
    "    else:break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that scrapes through all the reviews and returns the 1000 most common words with their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ignore capitalization and remove punctuation\n",
    "def scrape_text(dat):\n",
    "    common = ['i','these','the','im','am','has','this','they'\\\n",
    "              ,'in','my','are','on','that', 'and', 'a', 'to','it'\\\n",
    "              , 'is', 'for', 'of', 'but', 'have', 'not', 'them', 'with'\\\n",
    "              , 'so', 'very', 'was', 'as', 'you', 'like', 'size', 'wear'\\\n",
    "              , 'fit', 'be', 'great', 'just', 'would', 'comfortable'\\\n",
    "              , 'its', 'love', 'good', 'or', 'too', 'well', 'will'\\\n",
    "              , 'me', 'if', 'one', 'more', 'all', 'at']\n",
    "    wordCount = defaultdict(int)\n",
    "    punctuation = set(string.punctuation)\n",
    "    for d in dat:\n",
    "        #removing the punctuation and converting the review to a lowercase\n",
    "        r = ''.join([c for c in d[0].lower() if not c in punctuation])\n",
    "        #looping through the words in the review and incrementing the counts in the dictionary\n",
    "        for w in r.split():\n",
    "            wordCount[w] += 1\n",
    "\n",
    "    ### Just take the 1000 most popular words that are not in the common list\n",
    "    counts = [(wordCount[w], w) for w in wordCount if w not in common]\n",
    "    counts.sort()\n",
    "    counts.reverse()\n",
    "\n",
    "    words = [x[1] for x in counts[:1000]]\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets total frequency of top words \n",
    "tot_freq = 0 \n",
    "counts = scrape_text(datum_train)\n",
    "for wc in counts:\n",
    "    tot_freq += (wc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets the proportion of the counts for each word to the total counts of the top 1000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_500 = {}\n",
    "counts = scrape_text(datum_train)\n",
    "for wc in counts:\n",
    "    tup_one = wc[0] /tot_freq\n",
    "    tup_two = wc[1]\n",
    "    init_500[tup_two] = tup_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segregates the training data by the category of the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_train = []\n",
    "one_train = []\n",
    "two_train = []\n",
    "three_train = []\n",
    "four_train = []\n",
    "\n",
    "for d in datum_train:\n",
    "    cat = d[1]\n",
    "    if cat == 0:\n",
    "        zero_train.append(d)\n",
    "    if cat == 1:\n",
    "        one_train.append(d)\n",
    "    if cat == 2:\n",
    "        two_train.append(d)\n",
    "    if cat == 3:\n",
    "        three_train.append(d)\n",
    "    if cat == 4:\n",
    "        four_train.append(d)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to get the 1000 most common words unique to passed in dataset (each category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_words(ds):\n",
    "    tot_freq = 0 \n",
    "    counts = scrape_text(datum_train)\n",
    "    new_counts = scrape_text(ds)\n",
    "    \n",
    "    #get the total frequency \n",
    "    for w in new_counts:\n",
    "        tot_freq += w[0]\n",
    "\n",
    "    freq_500 = {}\n",
    "    \n",
    "    #getting frequency of each word \n",
    "    for w in new_counts:\n",
    "        tup_one = w[0] /tot_freq\n",
    "        tup_two = w[1]\n",
    "        freq_500[tup_two] = tup_one\n",
    "    #difference in category frequency and overall frequency\n",
    "    diff = {}\n",
    "    for k,v in freq_500.items():\n",
    "        diff[k] = v - init_500[k]\n",
    "    out = sorted(diff.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    return out[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bra', 0.001201640472737095),\n",
       " ('cute', 0.0007686408715002211),\n",
       " ('color', 0.0006266528756497802),\n",
       " ('dress', 0.000584970001109224),\n",
       " ('ordered', 0.000583195847440582),\n",
       " ('top', 0.0005704006338863449),\n",
       " ('small', 0.0005433016202854718),\n",
       " ('she', 0.0004898522538998007),\n",
       " ('because', 0.0004313221595723688),\n",
       " ('pretty', 0.0004193539199773945)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero = freq_words(zero_train)\n",
    "zero[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 0.004512844557778415),\n",
       " ('watch', 0.0033107358975127147),\n",
       " ('husband', 0.001974343116844607),\n",
       " ('his', 0.0014634362575765068),\n",
       " ('shirt', 0.001182299492066258),\n",
       " ('socks', 0.0011130854269565654),\n",
       " ('quality', 0.0010915085376884477),\n",
       " ('him', 0.0010431960450964842),\n",
       " ('belt', 0.0009705850453025548),\n",
       " ('shirts', 0.0008652482168458952)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one = freq_words(one_train)\n",
    "one[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 0.014442020834169402),\n",
       " ('her', 0.011677302863676242),\n",
       " ('daughter', 0.010927257186337063),\n",
       " ('old', 0.00586085881507134),\n",
       " ('year', 0.005118303693985239),\n",
       " ('loves', 0.004261394188629629),\n",
       " ('we', 0.0038995632729768824),\n",
       " ('cute', 0.003873264991969896),\n",
       " ('little', 0.0028266683090219765),\n",
       " ('dress', 0.0023693734782766706)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two = freq_words(two_train)\n",
    "two[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he', 0.015178964748182415),\n",
       " ('son', 0.0130553147303708),\n",
       " ('old', 0.007989844602986598),\n",
       " ('his', 0.006959016143503843),\n",
       " ('him', 0.006725615744352958),\n",
       " ('we', 0.006210344352673987),\n",
       " ('year', 0.006054674983862761),\n",
       " ('loves', 0.004791794803135574),\n",
       " ('costume', 0.0036515058157827905),\n",
       " ('boys', 0.0035508350847411247)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three = freq_words(three_train)\n",
    "three[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baby', 0.010781157840757054),\n",
       " ('her', 0.00774772770027755),\n",
       " ('cute', 0.006866010385753352),\n",
       " ('old', 0.006630260376600929),\n",
       " ('month', 0.005837247980458117),\n",
       " ('we', 0.005729618281405594),\n",
       " ('months', 0.005224189285835309),\n",
       " ('little', 0.00517018781335463),\n",
       " ('she', 0.004916463356276282),\n",
       " ('soft', 0.00456601973859075)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four = freq_words(four_train)\n",
    "four[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lists above we can see that the lists of each category seems to give us a fair indication of the category. <br> <br>\n",
    "We will now create a word set that contains the words from each of the four categories, we should expect the length of the list to be 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_word_set = zero + one + two + three + four"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a method to extract the x and y features for the data set. The y set will contain just the category of the item, while the x will have a 5005 features. The first 5000 features represent if the words in the mega word set are present in the review or not. While the next 5 features represent the number of items in each category that the user has bought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(ds, cat):\n",
    "    x_final= []\n",
    "    y_final =[]\n",
    "\n",
    "    for z in ds: \n",
    "        #append the category of the datapoint as our y value\n",
    "        y_final.append(z[1])\n",
    "        x = []\n",
    "        #I then check if each of the 5000 words are in the review\\\n",
    "        #appending 1 if yes and 0 if no, as the X feature\n",
    "        for wcc in final_word_set:\n",
    "            k = wcc[0]\n",
    "            if k in z[0]:\n",
    "                x.append(1)\n",
    "            else:\n",
    "                x.append(0)\n",
    "        ze=0\n",
    "        on=0\n",
    "        tw=0\n",
    "        th = 0\n",
    "        fo= 0\n",
    "        #getting the number of items in each category that user has bought\n",
    "        if z[2] in user_cat_d.keys():\n",
    "            for i in list(user_cat_d[z[2]]):\n",
    "                if i[1] == 0:\n",
    "                    ze+=1\n",
    "                if i[1] == 1:\n",
    "                    on+=1\n",
    "                if i[1] == 2:\n",
    "                    tw+=1\n",
    "                if i[1] == 3:\n",
    "                    th+= 1\n",
    "                if i[1] == 4:\n",
    "                    fo+= 1\n",
    "        x.append(ze)\n",
    "        x.append(on)\n",
    "        x.append(tw)\n",
    "        x.append(th)\n",
    "        x.append(fo)\n",
    "            \n",
    "        x_final.append(x)\n",
    "    return x_final, y_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a copy of the training and validation sets and shuffle the data to obtain more accurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = copy.deepcopy(datum_train)\n",
    "np.random.shuffle(dat)\n",
    "dat = dat[:int(len(dat)*1)]\n",
    "\n",
    "val = copy.deepcopy(datum_valid)\n",
    "np.random.shuffle(val)\n",
    "val = val[:int(len(dat)*1)]\n",
    "\n",
    "X_train, Y_train = get_xy(dat,0)\n",
    "X_val, Y_val = get_xy(val,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a LinearSVC on our data to predict the categories of items using our features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.01: 0.8754}\n",
      "{0.01: 0.8754, 0.1: 0.87684}\n",
      "{0.01: 0.8754, 0.1: 0.87684, 1: 0.87528}\n",
      "{0.01: 0.8754, 0.1: 0.87684, 1: 0.87528, 10: 0.873}\n"
     ]
    }
   ],
   "source": [
    "C = [0.01,0.1,1,10]\n",
    "scores = {}\n",
    "for c in C:\n",
    "    clf = svm.LinearSVC(C=c, multi_class = 'ovr', dual = False)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    scores[c] = clf.score(X_val, Y_val)\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.01: 0.8754, 0.1: 0.87684, 1: 0.87528, 10: 0.873}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our validation set to pick the value of C. From the accuracies calculated with different C values, we can see that we should pick C to be 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.LinearSVC(C=0.1, multi_class = 'ovr', dual = False)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9252\n",
      "0.87684\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(X_train, Y_train))\n",
    "print(clf.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy on the training set is __92.52%__ and our accuracy on the validation set is __87.68%__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells were used for the test data in the kaggle competition.<br>\n",
    "On the test set our accuracy was __88.128%__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data = []\n",
    "for l in readGz(\"test_Category.json.gz\"):\n",
    "    #print(l)\n",
    "    out_data.append([l['reviewText'],0,l['reviewerID']])\n",
    "\n",
    "    \n",
    "x,y = get_xy(out_data,0)\n",
    "out_cat = clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Category.txt\", 'w')\n",
    "predictions.write(\"reviewerID-reviewHash,category\\n\")\n",
    "for zipped_val in zip(out_cat,readGz(\"test_Category.json.gz\")):\n",
    "    cat = zipped_val[0]\n",
    "    l = zipped_val[1]\n",
    "    predictions.write(l['reviewerID'] + '-' + l['reviewHash'] + \",\" + str(cat) + \"\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
